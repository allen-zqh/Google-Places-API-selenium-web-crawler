{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219b3300",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee2d7fa",
   "metadata": {
    "code_folding": [
     17,
     49
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決算公告\n",
      "プライバシーポリシー\n",
      "情報セキュリティポリシー\n",
      "お知らせ一覧\n",
      "保険商品、取扱保険会社\n",
      "お問い合わせ\n",
      "保険部門関連「各保険会社への事故・災害等の連絡先」\n",
      "事業所一覧\n",
      "社是・経営理念\n",
      "新卒採用情報\n",
      "第160期「貸借対照表」「損益計算書」を公開しました\n",
      "キャリア採用情報\n",
      "Web購買\n",
      "事業内容紹介\n",
      "採用情報\n",
      "「金融商品の販売等に関する法律」に基づく勧誘方針\n",
      "会社概要\n",
      "関工商事株式会社\n",
      "「Web購買」は登録した会社様の専用サイトです。一般のお客様はお問い合せからお気軽にご連絡ください。\n",
      "個人情報保護方針\n",
      "障がい者採用情報\n",
      "トップメッセージ\n",
      "関電工グループ\n",
      "沿革\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [07:12, 432.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイトマップ\n",
      "恒温槽　流し台\n",
      "第24回日本言語聴覚学会　機器展示ご来場御礼\n",
      "フットケア・ハンドケア商品\n",
      "製品カタログ\n",
      "第90回 日本消化器内視鏡技師学会　機器展示　ご来場御礼\n",
      "新製品「点滴モニタリングシステム・モニドロップ」\n",
      "電動式人工喉頭\n",
      "移乗システム「ロールボード」\n",
      "日本麻酔科学会　第70回学術集会　機器展示　ご来場御礼\n",
      "第9回日本医療安全学会学術総会　機器展示　ご来場御礼\n",
      "センシンメディカルについて\n",
      "ステンレス製品 (特注承ります)\n",
      "商品紹介サイト（外部サイト）\n",
      "硬性挿管用喉頭鏡「エアトラック」\n",
      "日本臨床麻酔学会　第42回大会　機器展示　ご来場御礼\n",
      "関連リンク\n",
      "個人情報保護方針\n",
      "製品情報\n",
      "適格請求書発行事業者登録番号のお知らせ\n",
      "ホーム\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [13:14, 390.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイトマップ\n",
      "プライバシーポリシー\n",
      "日本語\n",
      "トップページ\n",
      "English\n",
      "卓上内視鏡洗浄消毒器EFEWD\n",
      "News\n",
      "2\n",
      "お問い合わせ\n",
      "お役⽴ち資料\n",
      "企業情報\n",
      "価格改定\n",
      "ISO 認証・許可証\n",
      "医工連携・研究について\n",
      "News一覧へ\n",
      "福祉機器\n",
      "披裂軟骨内転術用持針器 Ｋスティッチ\n",
      "採用情報\n",
      "鼻孔プロテクター\n",
      "ショールーム\n",
      "会社概要\n",
      "難治性めまいへの弛まぬ挑戦\n",
      "ファーストスコープ０８型\n",
      "「夏季休業および出荷のご案内」\n",
      "3\n",
      "透明性ガイドライン\n",
      "医療機器\n",
      "年末年始休業および出荷のご案内\n",
      "1\n",
      "トップメッセージ\n",
      "ZAOSONiCの鼻科手術への応用\n",
      "製品情報\n",
      "会社案内\n",
      "4\n",
      "沿革\n",
      "クリスタルアート\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [23:43, 474.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.common.by import By\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "path_to_chromedriver = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/geckodriver'\n",
    "out_path = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/result/'\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_clickable_text(elements):\n",
    "    all_clickable_tag = [i.text for i in elements]\n",
    "    all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "    #return list(set(all_clickable_tag + all_clickable_cls))\n",
    "    return list(set(all_clickable_tag))\n",
    "\n",
    "\n",
    "class WebPage():\n",
    "    \"\"\"\n",
    "    A model of a webpage\n",
    "    \"\"\"\n",
    "    def __init__(self, url, browser):\n",
    "        self.url = url\n",
    "        #self.url_domain = urlparse(url).netloc\n",
    "        try:\n",
    "            #self.browser = get_driver()\n",
    "            browser.get(url)\n",
    "            time.sleep(4)\n",
    "            #self.current_url = browser.current_url.copy()\n",
    "            #self.url_domain = urlparse(self.current_url).netloc\n",
    "            self.browser_start = 1\n",
    "            if url == 'http://mltrons.com/':\n",
    "                self.browser_start = None\n",
    "        except Exception:\n",
    "            self.browser_start = None\n",
    "            self.current_url = None\n",
    "            \n",
    "    @property\n",
    "    def homepage(self):\n",
    "        return browser.current_url\n",
    "            \n",
    "    @property\n",
    "    def all_clickable(self):\n",
    "        all_clickable_tag = browser.find_elements(By.TAG_NAME,\"a\")\n",
    "        all_clickable_tag = [i.text for i in all_clickable_tag]\n",
    "        all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "        return list(set(all_clickable_tag))\n",
    "    \n",
    "    \n",
    "    def URLandContent(self, browser):\n",
    "        if self.browser_start is None:\n",
    "            page_urls, page_texts, page_urls_all = [], [], []\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "        else:\n",
    "            page_urls, page_texts, page_urls_all = self._URLandContent(browser)\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def _URLandContent(self, browser):\n",
    "        if len(self.all_clickable) >= 1:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = self.URLandContentByClick(browser)\n",
    "        else:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = [], [], []\n",
    "            self.homepage_current_url = self.homepage\n",
    "            self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = self.URLandContentFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = [], [], []\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = self.URLandContentIFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = [], [], []\n",
    "        page_urls = page_urls_click + page_urls_frame + page_urls_iframe\n",
    "        page_texts = page_texts_click + page_texts_frame + page_texts_iframe\n",
    "        page_urls_all = page_urls_all_click + page_urls_all_frame + page_urls_all_iframe\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "        \n",
    "    def URLandContentByClick(self, browser):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        all_clickable = self.all_clickable\n",
    "        self.homepage_current_url = self.homepage\n",
    "        self.url_domain = urlparse(self.homepage_current_url).netloc\n",
    "        self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        \n",
    "        for element in all_clickable:\n",
    "            print(element)\n",
    "            try:\n",
    "                element = browser.find_element_by_link_text(element)\n",
    "                browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                browser.execute_script(\"arguments[0].click();\", element)\n",
    "                time.sleep(10)\n",
    "                current_page_url = browser.current_url\n",
    "                page_urls_all.append(current_page_url)\n",
    "                if similar(urlparse(current_page_url).netloc, self.url_domain) >= 0.75:\n",
    "                    page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                    page_urls.append(current_page_url)\n",
    "                #browser.back()\n",
    "                browser.get(self.homepage_current_url)\n",
    "                time.sleep(5)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    \n",
    "    def URLandContentFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def URLandContentIFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        #frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "#%%\n",
    "data_redo_lang = pd.read_csv('/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/crawl_test.csv')\n",
    "data_redo_lang['hojin_id'] = data_redo_lang['hojin_id'].astype(int)\n",
    "data_redo_lang = data_redo_lang.dropna()\n",
    "hojin_ids = data_redo_lang['hojin_id']\n",
    "url_webs = data_redo_lang['url_web']\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for hojin_id, url_web in tqdm(zip(hojin_ids, url_webs)):\n",
    "    hojin_id = int(hojin_id)\n",
    "    try:\n",
    "        browser = webdriver.Firefox(executable_path = path_to_chromedriver)\n",
    "        browser.set_page_load_timeout(15)\n",
    "        browser.set_script_timeout(15)\n",
    "        UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15'\n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.add_argument('--user-agent=' + UA)\n",
    "        #options.add_argument(\"--headless\")\n",
    "        try:\n",
    "            web = WebPage(url_web, browser)\n",
    "            time.sleep(4)\n",
    "            temp = web.all_clickable\n",
    "            page_urls, page_texts, page_urls_all = web.URLandContent(browser)\n",
    "            homepage_content = web.homepage_content\n",
    "            \n",
    "            content = {\n",
    "            'hojin_id': hojin_id,\n",
    "            'homepage_content': homepage_content,\n",
    "            'page_content': page_texts\n",
    "        }\n",
    "        except Exception:\n",
    "            content = {\n",
    "            'hojin_id': hojin_id,\n",
    "            'homepage_content': None,\n",
    "            'page_content': None\n",
    "        }\n",
    "\n",
    "        browser.close()\n",
    "        result_data.append(content)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"error\")\n",
    "\n",
    "results_df = pd.DataFrame(result_data)\n",
    "results_df.to_csv('/Users/zqh/Desktop/test.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f73ae4",
   "metadata": {},
   "source": [
    "# Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7d8da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/crawl_test.csv')\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "\n",
    "def extract_links_from_webpage(soup, base_url, main_domain):\n",
    "    links = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        full_url = urljoin(base_url, href)\n",
    "        \n",
    "        if is_valid_url(full_url):\n",
    "            similarity = url_similarity(main_domain, full_url)\n",
    "            if similarity > 0.7:\n",
    "                links.append(full_url)\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_webpage_and_links_text(url):\n",
    "    if not url or url != url:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=15, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            print(f\"HTTP 403 Forbidden for URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # homepage (1st layer)\n",
    "        first_layer_content = soup.get_text()\n",
    "        main_domain = urlparse(url).netloc\n",
    "        \n",
    "        # 2nd layers\n",
    "        second_layer_links = extract_links_from_webpage(soup, url, main_domain)\n",
    "        second_layer_content = \"\"\n",
    "        filtered_links = list(set(np.unique(second_layer_links)) - set(main_domain))\n",
    "        \n",
    "        for link in filtered_links:\n",
    "            try:\n",
    "                response = requests.get(link, timeout=15, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                if response.status_code == 403:\n",
    "                    print(f\"HTTP 403 Forbidden for URL: {link}\")\n",
    "                else:\n",
    "                    second_layer_content += BeautifulSoup(response.text, 'html.parser').get_text()\n",
    "            except Exception as e:\n",
    "                print(f\"无法获取网页内容：{str(e)}\")\n",
    "        \n",
    "        return first_layer_content + second_layer_content\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取网页内容：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "def url_similarity(url1, url2):\n",
    "    return difflib.SequenceMatcher(None, url1, url2).ratio()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return bool(parsed_url.scheme) and bool(parsed_url.netloc)\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    df.loc[index, 'content'] = get_webpage_and_links_text(row['url_web'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bd8125b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hojin_id</th>\n",
       "      <th>url_web</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.210000e+09</td>\n",
       "      <td>http://www.koshinmilk.co.jp/environmental.html</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n環境保全活動 - コーシン乳業株式会社\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hojin_id                                         url_web  \\\n",
       "0  1.210000e+09  http://www.koshinmilk.co.jp/environmental.html   \n",
       "\n",
       "                                             content  \n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n環境保全活動 - コーシン乳業株式会社\\n\\n\\n\\n\\...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e26323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＞＞詳細はお知らせをご覧ください。\n",
      "貸会議室・貸ホール\n",
      "お知らせ一覧\n",
      "営業所紹介\n",
      "会計事務所博覧会2023に出展します。\n",
      "おすすめ情報一覧\n",
      "OA機器・オフィスレイアウト\n",
      "登録商標・商標\n",
      "最上楽農園　夏の新メニュー　第二弾　ＮＥＷ！\n",
      "総合問い合わせ\n",
      "会計事務所のITサポート\n",
      "ホーム\n",
      "採用情報\n",
      "【ホームページが生まれ変わる！作成支援サービスをご利用ください】ゆりかご倶楽部ニュース8月号\n",
      "他のサービス\n",
      "会計事務所のオフィスプランニング\n",
      "会長挨拶\n",
      "【税理士実践塾】専門知識が身に付く実践セミナーのご案内\n",
      "サイトマップ\n",
      "開業をお考えの方へ\n",
      "総合支援サイト\n",
      "個人情報保護方針　及び　特定個人情報取扱基本方針\n",
      "会社概要\n",
      "エッサムファミリー会 会報 2023年10月号を公開しました。\n",
      "貸会議室\n",
      "注意喚起：当社社員を騙る不審なメールにご注意ください\n",
      "台風7号の影響によるお荷物のお届けについて\n",
      "財務・税務システム\n",
      "税理士業務書式文例集(令和5年改訂版)のご案内\n",
      "過去の情報を見る>>>\n",
      "沿革\n",
      "おトクなキャンペーン一覧\n",
      "情報セキュリティ基本方針\n",
      "コロナ対策　オフィスの創り方　冊子\n",
      "事務用品\n",
      "エッサムファミリー会\n",
      "会計士・税理士向け総合支援情報サイト\n",
      "会長経歴・講演履歴\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [10:35, 635.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:Message: Unable to locate element: /html/body\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [10:47, 268.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スキップしてコンテンツに移動する\n",
      "the button’s\n",
      "INFORMATION\n",
      "INFORMATION\n",
      "個人情報保護方針\n",
      "■新規会員登録はこちら■\n",
      "COMPANY\n",
      "SAMPLE BOOK\n",
      "SAMPLE BOOK\n",
      "お問合せ(幸徳ボタン)\n",
      "検索\n",
      "BLOG\n",
      "BLOG\n",
      "お問合せ(the button's)\n",
      "PRODUCTS\n",
      "特定商取引表記\n",
      "ページ内検索\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [15:15, 305.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.common.by import By\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "path_to_chromedriver = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/geckodriver'\n",
    "out_path = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/result/'\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_clickable_text(elements):\n",
    "    all_clickable_tag = [i.text for i in elements]\n",
    "    all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "    #return list(set(all_clickable_tag + all_clickable_cls))\n",
    "    return list(set(all_clickable_tag))\n",
    "\n",
    "\n",
    "class WebPage():\n",
    "    \"\"\"\n",
    "    A model of a webpage\n",
    "    \"\"\"\n",
    "    def __init__(self, url, browser):\n",
    "        self.url = url\n",
    "        #self.url_domain = urlparse(url).netloc\n",
    "        try:\n",
    "            #self.browser = get_driver()\n",
    "            browser.get(url)\n",
    "            time.sleep(4)\n",
    "            #self.current_url = browser.current_url.copy()\n",
    "            #self.url_domain = urlparse(self.current_url).netloc\n",
    "            self.browser_start = 1\n",
    "            if url == 'http://mltrons.com/':\n",
    "                self.browser_start = None\n",
    "        except Exception:\n",
    "            self.browser_start = None\n",
    "            self.current_url = None\n",
    "            \n",
    "    @property\n",
    "    def homepage(self):\n",
    "        return browser.current_url\n",
    "            \n",
    "    @property\n",
    "    def all_clickable(self):\n",
    "        all_clickable_tag = browser.find_elements(By.TAG_NAME,\"a\")\n",
    "        all_clickable_tag = [i.text for i in all_clickable_tag]\n",
    "        all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "        return list(set(all_clickable_tag))\n",
    "    \n",
    "    \n",
    "    def URLandContent(self, browser):\n",
    "        if self.browser_start is None:\n",
    "            page_urls, page_texts, page_urls_all = [], [], []\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "        else:\n",
    "            page_urls, page_texts, page_urls_all = self._URLandContent(browser)\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def _URLandContent(self, browser):\n",
    "        if len(self.all_clickable) >= 1:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = self.URLandContentByClick(browser)\n",
    "        else:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = [], [], []\n",
    "            self.homepage_current_url = self.homepage\n",
    "            self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = self.URLandContentFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = [], [], []\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = self.URLandContentIFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = [], [], []\n",
    "        page_urls = page_urls_click + page_urls_frame + page_urls_iframe\n",
    "        page_texts = page_texts_click + page_texts_frame + page_texts_iframe\n",
    "        page_urls_all = page_urls_all_click + page_urls_all_frame + page_urls_all_iframe\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "        \n",
    "    def URLandContentByClick(self, browser):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        all_clickable = self.all_clickable\n",
    "        self.homepage_current_url = self.homepage\n",
    "        self.url_domain = urlparse(self.homepage_current_url).netloc\n",
    "        self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        \n",
    "        for element in all_clickable:\n",
    "            print(element)\n",
    "            try:\n",
    "                element = browser.find_element_by_link_text(element)\n",
    "                browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                browser.execute_script(\"arguments[0].click();\", element)\n",
    "                time.sleep(10)\n",
    "                current_page_url = browser.current_url\n",
    "                page_urls_all.append(current_page_url)\n",
    "                if similar(urlparse(current_page_url).netloc, self.url_domain) >= 0.75:\n",
    "                    page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                    page_urls.append(current_page_url)\n",
    "                #browser.back()\n",
    "                browser.get(self.homepage_current_url)\n",
    "                time.sleep(5)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    \n",
    "    def URLandContentFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def URLandContentIFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        #frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "#%%\n",
    "data_redo_lang = pd.read_csv('/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/crawl_test.csv')\n",
    "data_redo_lang['hojin_id'] = data_redo_lang['hojin_id'].astype(int)\n",
    "data_redo_lang = data_redo_lang.dropna()\n",
    "hojin_ids = data_redo_lang['hojin_id']\n",
    "url_webs = data_redo_lang['url_web']\n",
    "\n",
    "for hojin_id, url_web in tqdm(zip(hojin_ids, url_webs)):\n",
    "    hojin_id = int(hojin_id)\n",
    "    try:\n",
    "        browser = webdriver.Firefox(executable_path = path_to_chromedriver)\n",
    "        browser.set_page_load_timeout(15)\n",
    "        browser.set_script_timeout(15)\n",
    "        UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15'\n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.add_argument('--user-agent=' + UA)\n",
    "        #options.add_argument(\"--headless\")\n",
    "        try:\n",
    "            web = WebPage(url_web, browser)\n",
    "            time.sleep(4)\n",
    "            temp = web.all_clickable\n",
    "            page_urls, page_texts, page_urls_all = web.URLandContent(browser)\n",
    "            homepage_content = web.homepage_content\n",
    "            combined_page_texts = '\\n'.join(page_texts)\n",
    "            full_content = f'{homepage_content}\\n\\n{combined_page_texts}'\n",
    "            \n",
    "            hojin_id_str = str(hojin_id)\n",
    "            output_file = os.path.join(out_path, f'{hojin_id_str}.txt')\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error:{str(e)}\")\n",
    "            \n",
    "        browser.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error:{str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "528.852px",
    "left": "441px",
    "right": "20px",
    "top": "109px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
