{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219b3300",
   "metadata": {},
   "source": [
    "# 具体的爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee2d7fa",
   "metadata": {
    "code_folding": [
     17,
     49
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決算公告\n",
      "プライバシーポリシー\n",
      "情報セキュリティポリシー\n",
      "お知らせ一覧\n",
      "保険商品、取扱保険会社\n",
      "お問い合わせ\n",
      "保険部門関連「各保険会社への事故・災害等の連絡先」\n",
      "事業所一覧\n",
      "社是・経営理念\n",
      "新卒採用情報\n",
      "第160期「貸借対照表」「損益計算書」を公開しました\n",
      "キャリア採用情報\n",
      "Web購買\n",
      "事業内容紹介\n",
      "採用情報\n",
      "「金融商品の販売等に関する法律」に基づく勧誘方針\n",
      "会社概要\n",
      "関工商事株式会社\n",
      "「Web購買」は登録した会社様の専用サイトです。一般のお客様はお問い合せからお気軽にご連絡ください。\n",
      "個人情報保護方針\n",
      "障がい者採用情報\n",
      "トップメッセージ\n",
      "関電工グループ\n",
      "沿革\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [07:12, 432.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイトマップ\n",
      "恒温槽　流し台\n",
      "第24回日本言語聴覚学会　機器展示ご来場御礼\n",
      "フットケア・ハンドケア商品\n",
      "製品カタログ\n",
      "第90回 日本消化器内視鏡技師学会　機器展示　ご来場御礼\n",
      "新製品「点滴モニタリングシステム・モニドロップ」\n",
      "電動式人工喉頭\n",
      "移乗システム「ロールボード」\n",
      "日本麻酔科学会　第70回学術集会　機器展示　ご来場御礼\n",
      "第9回日本医療安全学会学術総会　機器展示　ご来場御礼\n",
      "センシンメディカルについて\n",
      "ステンレス製品 (特注承ります)\n",
      "商品紹介サイト（外部サイト）\n",
      "硬性挿管用喉頭鏡「エアトラック」\n",
      "日本臨床麻酔学会　第42回大会　機器展示　ご来場御礼\n",
      "関連リンク\n",
      "個人情報保護方針\n",
      "製品情報\n",
      "適格請求書発行事業者登録番号のお知らせ\n",
      "ホーム\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [13:14, 390.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイトマップ\n",
      "プライバシーポリシー\n",
      "日本語\n",
      "トップページ\n",
      "English\n",
      "卓上内視鏡洗浄消毒器EFEWD\n",
      "News\n",
      "2\n",
      "お問い合わせ\n",
      "お役⽴ち資料\n",
      "企業情報\n",
      "価格改定\n",
      "ISO 認証・許可証\n",
      "医工連携・研究について\n",
      "News一覧へ\n",
      "福祉機器\n",
      "披裂軟骨内転術用持針器 Ｋスティッチ\n",
      "採用情報\n",
      "鼻孔プロテクター\n",
      "ショールーム\n",
      "会社概要\n",
      "難治性めまいへの弛まぬ挑戦\n",
      "ファーストスコープ０８型\n",
      "「夏季休業および出荷のご案内」\n",
      "3\n",
      "透明性ガイドライン\n",
      "医療機器\n",
      "年末年始休業および出荷のご案内\n",
      "1\n",
      "トップメッセージ\n",
      "ZAOSONiCの鼻科手術への応用\n",
      "製品情報\n",
      "会社案内\n",
      "4\n",
      "沿革\n",
      "クリスタルアート\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [23:43, 474.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.common.by import By\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "path_to_chromedriver = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/geckodriver'\n",
    "out_path = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/result/'\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_clickable_text(elements):\n",
    "    all_clickable_tag = [i.text for i in elements]\n",
    "    all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "    #return list(set(all_clickable_tag + all_clickable_cls))\n",
    "    return list(set(all_clickable_tag))\n",
    "\n",
    "\n",
    "class WebPage():\n",
    "    \"\"\"\n",
    "    A model of a webpage\n",
    "    \"\"\"\n",
    "    def __init__(self, url, browser):\n",
    "        self.url = url\n",
    "        #self.url_domain = urlparse(url).netloc\n",
    "        try:\n",
    "            #self.browser = get_driver()\n",
    "            browser.get(url)\n",
    "            time.sleep(4)\n",
    "            #self.current_url = browser.current_url.copy()\n",
    "            #self.url_domain = urlparse(self.current_url).netloc\n",
    "            self.browser_start = 1\n",
    "            if url == 'http://mltrons.com/':\n",
    "                self.browser_start = None\n",
    "        except Exception:\n",
    "            self.browser_start = None\n",
    "            self.current_url = None\n",
    "            \n",
    "    @property\n",
    "    def homepage(self):\n",
    "        return browser.current_url\n",
    "            \n",
    "    @property\n",
    "    def all_clickable(self):\n",
    "        all_clickable_tag = browser.find_elements(By.TAG_NAME,\"a\")\n",
    "        all_clickable_tag = [i.text for i in all_clickable_tag]\n",
    "        all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "        return list(set(all_clickable_tag))\n",
    "    \n",
    "    \n",
    "    def URLandContent(self, browser):\n",
    "        if self.browser_start is None:\n",
    "            page_urls, page_texts, page_urls_all = [], [], []\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "        else:\n",
    "            page_urls, page_texts, page_urls_all = self._URLandContent(browser)\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def _URLandContent(self, browser):\n",
    "        if len(self.all_clickable) >= 1:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = self.URLandContentByClick(browser)\n",
    "        else:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = [], [], []\n",
    "            self.homepage_current_url = self.homepage\n",
    "            self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = self.URLandContentFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = [], [], []\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = self.URLandContentIFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = [], [], []\n",
    "        page_urls = page_urls_click + page_urls_frame + page_urls_iframe\n",
    "        page_texts = page_texts_click + page_texts_frame + page_texts_iframe\n",
    "        page_urls_all = page_urls_all_click + page_urls_all_frame + page_urls_all_iframe\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "        \n",
    "    def URLandContentByClick(self, browser):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        all_clickable = self.all_clickable\n",
    "        self.homepage_current_url = self.homepage\n",
    "        self.url_domain = urlparse(self.homepage_current_url).netloc\n",
    "        self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        \n",
    "        for element in all_clickable:\n",
    "            print(element)\n",
    "            try:\n",
    "                element = browser.find_element_by_link_text(element)\n",
    "                browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                browser.execute_script(\"arguments[0].click();\", element)\n",
    "                time.sleep(10)\n",
    "                current_page_url = browser.current_url\n",
    "                page_urls_all.append(current_page_url)\n",
    "                if similar(urlparse(current_page_url).netloc, self.url_domain) >= 0.75:\n",
    "                    page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                    page_urls.append(current_page_url)\n",
    "                #browser.back()\n",
    "                browser.get(self.homepage_current_url)\n",
    "                time.sleep(5)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    \n",
    "    def URLandContentFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def URLandContentIFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        #frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "#%%\n",
    "data_redo_lang = pd.read_csv('/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/crawl_test.csv')[0:1]\n",
    "data_redo_lang['hojin_id'] = data_redo_lang['hojin_id'].astype(int)\n",
    "data_redo_lang = data_redo_lang.dropna()\n",
    "hojin_ids = data_redo_lang['hojin_id']\n",
    "url_webs = data_redo_lang['url_web']\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for hojin_id, url_web in tqdm(zip(hojin_ids, url_webs)):\n",
    "    hojin_id = int(hojin_id)\n",
    "    try:\n",
    "        browser = webdriver.Firefox(executable_path = path_to_chromedriver)\n",
    "        browser.set_page_load_timeout(15)\n",
    "        browser.set_script_timeout(15)\n",
    "        UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15'\n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.add_argument('--user-agent=' + UA)\n",
    "        #options.add_argument(\"--headless\")\n",
    "        try:\n",
    "            web = WebPage(url_web, browser)\n",
    "            time.sleep(4)\n",
    "            temp = web.all_clickable\n",
    "            page_urls, page_texts, page_urls_all = web.URLandContent(browser)\n",
    "            homepage_content = web.homepage_content\n",
    "            \n",
    "            content = {\n",
    "            'hojin_id': hojin_id,\n",
    "            'homepage_content': homepage_content,\n",
    "            'page_content': page_texts\n",
    "        }\n",
    "        except Exception:\n",
    "            content = {\n",
    "            'hojin_id': hojin_id,\n",
    "            'homepage_content': None,\n",
    "            'page_content': None\n",
    "        }\n",
    "\n",
    "        browser.close()\n",
    "        result_data.append(content)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"error\")\n",
    "\n",
    "results_df = pd.DataFrame(result_data)\n",
    "results_df.to_csv('/Users/zqh/Desktop/111.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f73ae4",
   "metadata": {},
   "source": [
    "# 简单的爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7d8da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/crawl_test.csv')\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "\n",
    "def extract_links_from_webpage(soup, base_url, main_domain):\n",
    "    links = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        full_url = urljoin(base_url, href)\n",
    "        \n",
    "        if is_valid_url(full_url):\n",
    "            similarity = url_similarity(main_domain, full_url)\n",
    "            if similarity > 0.7:\n",
    "                links.append(full_url)\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_webpage_and_links_text(url):\n",
    "    if not url or url != url:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=15, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            print(f\"HTTP 403 Forbidden for URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # homepage (1st layer)\n",
    "        first_layer_content = soup.get_text()\n",
    "        main_domain = urlparse(url).netloc\n",
    "        \n",
    "        # 2nd layers\n",
    "        second_layer_links = extract_links_from_webpage(soup, url, main_domain)\n",
    "        second_layer_content = \"\"\n",
    "        filtered_links = list(set(np.unique(second_layer_links)) - set(main_domain))\n",
    "        \n",
    "        for link in filtered_links:\n",
    "            try:\n",
    "                response = requests.get(link, timeout=15, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                if response.status_code == 403:\n",
    "                    print(f\"HTTP 403 Forbidden for URL: {link}\")\n",
    "                else:\n",
    "                    second_layer_content += BeautifulSoup(response.text, 'html.parser').get_text()\n",
    "            except Exception as e:\n",
    "                print(f\"无法获取网页内容：{str(e)}\")\n",
    "        \n",
    "        return first_layer_content + second_layer_content\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取网页内容：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "def url_similarity(url1, url2):\n",
    "    return difflib.SequenceMatcher(None, url1, url2).ratio()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return bool(parsed_url.scheme) and bool(parsed_url.netloc)\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    df.loc[index, 'content'] = get_webpage_and_links_text(row['url_web'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bd8125b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hojin_id</th>\n",
       "      <th>url_web</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.210000e+09</td>\n",
       "      <td>http://www.koshinmilk.co.jp/environmental.html</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n環境保全活動 - コーシン乳業株式会社\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hojin_id                                         url_web  \\\n",
       "0  1.210000e+09  http://www.koshinmilk.co.jp/environmental.html   \n",
       "\n",
       "                                             content  \n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n環境保全活動 - コーシン乳業株式会社\\n\\n\\n\\n\\...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e26323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＞＞詳細はお知らせをご覧ください。\n",
      "貸会議室・貸ホール\n",
      "お知らせ一覧\n",
      "営業所紹介\n",
      "会計事務所博覧会2023に出展します。\n",
      "おすすめ情報一覧\n",
      "OA機器・オフィスレイアウト\n",
      "登録商標・商標\n",
      "最上楽農園　夏の新メニュー　第二弾　ＮＥＷ！\n",
      "総合問い合わせ\n",
      "会計事務所のITサポート\n",
      "ホーム\n",
      "採用情報\n",
      "【ホームページが生まれ変わる！作成支援サービスをご利用ください】ゆりかご倶楽部ニュース8月号\n",
      "他のサービス\n",
      "会計事務所のオフィスプランニング\n",
      "会長挨拶\n",
      "【税理士実践塾】専門知識が身に付く実践セミナーのご案内\n",
      "サイトマップ\n",
      "開業をお考えの方へ\n",
      "総合支援サイト\n",
      "個人情報保護方針　及び　特定個人情報取扱基本方針\n",
      "会社概要\n",
      "エッサムファミリー会 会報 2023年10月号を公開しました。\n",
      "貸会議室\n",
      "注意喚起：当社社員を騙る不審なメールにご注意ください\n",
      "台風7号の影響によるお荷物のお届けについて\n",
      "財務・税務システム\n",
      "税理士業務書式文例集(令和5年改訂版)のご案内\n",
      "過去の情報を見る>>>\n",
      "沿革\n",
      "おトクなキャンペーン一覧\n",
      "情報セキュリティ基本方針\n",
      "コロナ対策　オフィスの創り方　冊子\n",
      "事務用品\n",
      "エッサムファミリー会\n",
      "会計士・税理士向け総合支援情報サイト\n",
      "会長経歴・講演履歴\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [10:35, 635.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:Message: Unable to locate element: /html/body\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [10:47, 268.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スキップしてコンテンツに移動する\n",
      "the button’s\n",
      "INFORMATION\n",
      "INFORMATION\n",
      "個人情報保護方針\n",
      "■新規会員登録はこちら■\n",
      "COMPANY\n",
      "SAMPLE BOOK\n",
      "SAMPLE BOOK\n",
      "お問合せ(幸徳ボタン)\n",
      "検索\n",
      "BLOG\n",
      "BLOG\n",
      "お問合せ(the button's)\n",
      "PRODUCTS\n",
      "特定商取引表記\n",
      "ページ内検索\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [15:15, 305.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.common.by import By\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "path_to_chromedriver = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/geckodriver'\n",
    "out_path = '/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/result/'\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_clickable_text(elements):\n",
    "    all_clickable_tag = [i.text for i in elements]\n",
    "    all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "    #return list(set(all_clickable_tag + all_clickable_cls))\n",
    "    return list(set(all_clickable_tag))\n",
    "\n",
    "\n",
    "class WebPage():\n",
    "    \"\"\"\n",
    "    A model of a webpage\n",
    "    \"\"\"\n",
    "    def __init__(self, url, browser):\n",
    "        self.url = url\n",
    "        #self.url_domain = urlparse(url).netloc\n",
    "        try:\n",
    "            #self.browser = get_driver()\n",
    "            browser.get(url)\n",
    "            time.sleep(4)\n",
    "            #self.current_url = browser.current_url.copy()\n",
    "            #self.url_domain = urlparse(self.current_url).netloc\n",
    "            self.browser_start = 1\n",
    "            if url == 'http://mltrons.com/':\n",
    "                self.browser_start = None\n",
    "        except Exception:\n",
    "            self.browser_start = None\n",
    "            self.current_url = None\n",
    "            \n",
    "    @property\n",
    "    def homepage(self):\n",
    "        return browser.current_url\n",
    "            \n",
    "    @property\n",
    "    def all_clickable(self):\n",
    "        all_clickable_tag = browser.find_elements(By.TAG_NAME,\"a\")\n",
    "        all_clickable_tag = [i.text for i in all_clickable_tag]\n",
    "        all_clickable_tag = [i for i in all_clickable_tag if i != '' and not i.isspace()]\n",
    "        return list(set(all_clickable_tag))\n",
    "    \n",
    "    \n",
    "    def URLandContent(self, browser):\n",
    "        if self.browser_start is None:\n",
    "            page_urls, page_texts, page_urls_all = [], [], []\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "        else:\n",
    "            page_urls, page_texts, page_urls_all = self._URLandContent(browser)\n",
    "            return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def _URLandContent(self, browser):\n",
    "        if len(self.all_clickable) >= 1:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = self.URLandContentByClick(browser)\n",
    "        else:\n",
    "            page_urls_click, page_texts_click, page_urls_all_click = [], [], []\n",
    "            self.homepage_current_url = self.homepage\n",
    "            self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = self.URLandContentFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_frame, page_texts_frame, page_urls_all_frame = [], [], []\n",
    "        browser.get(self.url)\n",
    "        time.sleep(5)\n",
    "        frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        frames = [frame.get_attribute('src') for frame in frames]\n",
    "        frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        if len(frames) >= 1:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = self.URLandContentIFrame(browser, frames)\n",
    "        else:\n",
    "            page_urls_iframe, page_texts_iframe, page_urls_all_iframe = [], [], []\n",
    "        page_urls = page_urls_click + page_urls_frame + page_urls_iframe\n",
    "        page_texts = page_texts_click + page_texts_frame + page_texts_iframe\n",
    "        page_urls_all = page_urls_all_click + page_urls_all_frame + page_urls_all_iframe\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "        \n",
    "    def URLandContentByClick(self, browser):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        all_clickable = self.all_clickable\n",
    "        self.homepage_current_url = self.homepage\n",
    "        self.url_domain = urlparse(self.homepage_current_url).netloc\n",
    "        self.homepage_content = browser.find_element_by_xpath(\"/html/body\").text\n",
    "        \n",
    "        for element in all_clickable:\n",
    "            print(element)\n",
    "            try:\n",
    "                element = browser.find_element_by_link_text(element)\n",
    "                browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                browser.execute_script(\"arguments[0].click();\", element)\n",
    "                time.sleep(10)\n",
    "                current_page_url = browser.current_url\n",
    "                page_urls_all.append(current_page_url)\n",
    "                if similar(urlparse(current_page_url).netloc, self.url_domain) >= 0.75:\n",
    "                    page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                    page_urls.append(current_page_url)\n",
    "                #browser.back()\n",
    "                browser.get(self.homepage_current_url)\n",
    "                time.sleep(5)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    \n",
    "    def URLandContentFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'frame')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "    def URLandContentIFrame(self, browser, frames):\n",
    "        page_texts = []\n",
    "        page_urls = []\n",
    "        page_urls_all = []\n",
    "        #frames = browser.find_elements(By.TAG_NAME, 'iframe')\n",
    "        #frames = [frame.get_attribute('src') for frame in frames]\n",
    "        #frames = [frame for frame in frames if frame.startswith('http')]\n",
    "        for frame in frames:\n",
    "            browser.get(frame)\n",
    "            time.sleep(5)\n",
    "            all_clickable_frame = get_clickable_text(browser.find_elements(By.TAG_NAME, \"a\"))\n",
    "            for elem in all_clickable_frame:\n",
    "                try:\n",
    "                    element = browser.find_element_by_link_text(elem)\n",
    "                    current_page_url = element.get_attribute('href')\n",
    "                    page_urls_all.append(current_page_url)\n",
    "                    browser.execute_script(\"arguments[0].target='_self'\", element)\n",
    "                    browser.execute_script(\"arguments[0].click();\", element)\n",
    "                    time.sleep(5)\n",
    "                    if similar(urlparse(current_page_url).netloc, urlparse(frame).netloc) >= 0.75:\n",
    "                        page_texts.append(browser.find_element_by_xpath(\"/html/body\").text)\n",
    "                        page_urls.append(current_page_url)\n",
    "                    browser.get(frame)\n",
    "                    time.sleep(5)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return page_urls, page_texts, page_urls_all\n",
    "    \n",
    "#%%\n",
    "data_redo_lang = pd.read_csv('/Users/zqh/Documents/RIETI_new/RIETI_AI_Survey/output_data/crawl_test.csv')\n",
    "data_redo_lang['hojin_id'] = data_redo_lang['hojin_id'].astype(int)\n",
    "data_redo_lang = data_redo_lang.dropna()\n",
    "hojin_ids = data_redo_lang['hojin_id']\n",
    "url_webs = data_redo_lang['url_web']\n",
    "\n",
    "for hojin_id, url_web in tqdm(zip(hojin_ids, url_webs)):\n",
    "    hojin_id = int(hojin_id)\n",
    "    try:\n",
    "        browser = webdriver.Firefox(executable_path = path_to_chromedriver)\n",
    "        browser.set_page_load_timeout(15)\n",
    "        browser.set_script_timeout(15)\n",
    "        UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15'\n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.add_argument('--user-agent=' + UA)\n",
    "        #options.add_argument(\"--headless\")\n",
    "        try:\n",
    "            web = WebPage(url_web, browser)\n",
    "            time.sleep(4)\n",
    "            temp = web.all_clickable\n",
    "            page_urls, page_texts, page_urls_all = web.URLandContent(browser)\n",
    "            homepage_content = web.homepage_content\n",
    "            combined_page_texts = '\\n'.join(page_texts)\n",
    "            full_content = f'{homepage_content}\\n\\n{combined_page_texts}'\n",
    "            \n",
    "            hojin_id_str = str(hojin_id)\n",
    "            output_file = os.path.join(out_path, f'{hojin_id_str}.txt')\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error:{str(e)}\")\n",
    "            \n",
    "        browser.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error:{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "265d0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/zqh/Desktop/test.csv', dtype={'hojin_id': str})\n",
    "df['hojin_id'] = df['hojin_id'].str.strip()\n",
    "folder_path = '/Users/zqh/Desktop/a/'\n",
    "existing_hojin_ids = [file_name.split('.')[0] for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "df = df[~df['hojin_id'].isin(existing_hojin_ids)]\n",
    "df.to_csv('/Users/zqh/Desktop/redo_lenx.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c811dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='www.mcmap.co.jp', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:15,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.armani.com', port=80): Read timed out. (read timeout=15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:15,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.lloyds-japan.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f928af1e7f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.pearly-marusho.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f928af1e9a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='mail.shima-tra.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f928af1e460>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:16,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.jfomaimex.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92c8a0edc0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:16,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='www.smiths-medical.com', port=443): Max retries exceeded with url: /ja-jp/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92c8a0ee20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [00:31,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.seiko-watch.co.jp', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f92c8a0eb20>, 'Connection to www.seiko-watch.co.jp timed out. (connect timeout=15)'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='stagg.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92c8a13670>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:46,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='www.st.com', port=443): Read timed out. (read timeout=15)\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.lg-japan.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92c8a13eb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:01,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.tyj.co.jp', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f92c8a13100>, 'Connection to www.tyj.co.jp timed out. (connect timeout=15)'))\n",
      "无法获取网页内容：('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='catalina-jp.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1131)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [01:46, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='mundipharma.co.jp', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f92c8a13a90>, 'Connection to mundipharma.co.jp timed out. (connect timeout=15)'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.echelon.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92c8a0efd0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='www.industrial.ai', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92c8a0ec10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [02:05,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='www.aspenpharma.co.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbbf4c10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.stx-japan.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbbf4c40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='12ku.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbbf4c70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [02:12,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='cheers-inc.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbbf4c10>: Failed to establish a new connection: [Errno 51] Network is unreachable'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [02:28,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='www.dpdhl.com', port=443): Read timed out. (read timeout=15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [02:46,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.jal.co.jp', port=80): Read timed out. (read timeout=15)\n",
      "无法获取网页内容：HTTPSConnectionPool(host='ap.thermoking.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbce4c70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [02:50,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.kingsmen.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbcec460>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.kingsmen.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbcec670>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.kingsmen.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbcec160>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [03:05,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.grace-net.co.jp', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f92bbcec100>, 'Connection to www.grace-net.co.jp timed out. (connect timeout=15)'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='shinoh.ne.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbcecd00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [03:06,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.cr-soft.co.jo', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbceceb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [03:22,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.mycare.co.jp', port=80): Max retries exceeded with url: /user_data/company.php (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f92bbcecbb0>, 'Connection to www.mycare.co.jp timed out. (connect timeout=15)'))\n",
      "无法获取网页内容：Invalid URL 'http://????????.com/': No host supplied\n",
      "无法获取网页内容：HTTPSConnectionPool(host='tksaeki.saeki-selvahd.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbcec6a0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [03:22,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='jackdown.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbceca30>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [03:37,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.takanashi-milk.co.jp', port=80): Max retries exceeded with url: /company/gaiyou.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f92bbcec880>, 'Connection to www.takanashi-milk.co.jp timed out. (connect timeout=15)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [03:53,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPSConnectionPool(host='www.phoenixcontact.com', port=443): Read timed out. (read timeout=15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [03:54,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.sotetsu-kosan.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd10c40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='lloyds-register.co.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbd10a90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='www.abipro.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbd10ee0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='www.hakonetozan-hire.co.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbd10190>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [03:55,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.kanto-kasei-kogyo.co.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd01cd0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='www.gatten.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbd01f40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='life-hair-esthe.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd01df0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [03:55,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.funabashikousan.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd01d00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='leahome.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f92bbd01ca0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.staygold-net.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd01df0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [03:55,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.sjnk-ag.com', port=80): Max retries exceeded with url: /b/wing/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd01ca0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.suzuhiroco.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd10070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www6.ocn.ne.jp', port=80): Max retries exceeded with url: /~orange-g/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd10e20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.salon-coeur.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd10eb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='narumien.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd10a90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "59it [03:56,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='ayumien.or', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd5a490>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "无法获取网页内容：HTTPConnectionPool(host='www.honfleur.jp', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f92bbd5a670>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [04:12,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法获取网页内容：HTTPConnectionPool(host='www.sanden.co.jp', port=80): Max retries exceeded with url: /english/company/domestic.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f92bbd5d4f0>, 'Connection to www.sanden.co.jp timed out. (connect timeout=15)'))\n",
      "无法获取网页内容：HTTPSConnectionPool(host='www.idasogo.co.jp', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f928c06f4f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [04:14,  4.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[1;32m     78\u001b[0m     hojin_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(df\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhojin_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 79\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mget_webpage_and_links_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl_web\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhojin_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mget_webpage_and_links_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    396\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[1;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1254\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/http/client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1009\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m \n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/http/client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 951\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('/Users/zqh/Desktop/len0234_redo.csv')\n",
    "out_path = '/Users/zqh/Desktop/1/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "\n",
    "def extract_links_from_webpage(soup, base_url, main_domain):\n",
    "    links = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        full_url = urljoin(base_url, href)\n",
    "        \n",
    "        if is_valid_url(full_url):\n",
    "            similarity = url_similarity(main_domain, full_url)\n",
    "            if similarity > 0.7:\n",
    "                links.append(full_url)\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_webpage_and_links_text(url):\n",
    "    if not url or url != url:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=15, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            #print(f\"HTTP 403 Forbidden for URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # homepage (1st layer)\n",
    "        first_layer_content = soup.get_text()\n",
    "        main_domain = urlparse(url).netloc\n",
    "        \n",
    "        # 2nd layers\n",
    "        second_layer_links = extract_links_from_webpage(soup, url, main_domain)\n",
    "        second_layer_content = \"\"\n",
    "        filtered_links = [link for link in second_layer_links if urlparse(link).netloc != main_domain]\n",
    "        \n",
    "        for link in filtered_links:\n",
    "            try:\n",
    "                response = requests.get(link, timeout=15, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                if response.status_code == 403:\n",
    "                    #print(f\"HTTP 403 Forbidden for URL: {link}\")\n",
    "                else:\n",
    "                    second_layer_content += BeautifulSoup(response.text, 'html.parser').get_text()\n",
    "            except Exception as e:\n",
    "                #print(f\"无法获取网页内容：{str(e)}\")\n",
    "        \n",
    "        full_content = f'{first_layer_content}\\n\\n{second_layer_content}'\n",
    "        return full_content\n",
    "    except Exception as e:\n",
    "        #print(f\"无法获取网页内容：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "def url_similarity(url1, url2):\n",
    "    return difflib.SequenceMatcher(None, url1, url2).ratio()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return bool(parsed_url.scheme) and bool(parsed_url.netloc)\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    hojin_id = str(df.loc[index, 'hojin_id'])\n",
    "    content = get_webpage_and_links_text(row['url_web'])\n",
    "    if content is not None:\n",
    "        output_file = os.path.join(out_path, f'{hojin_id}.txt')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7dcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "528.852px",
    "left": "441px",
    "right": "20px",
    "top": "109px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
